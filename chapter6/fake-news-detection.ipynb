{"cells":[{"cell_type":"markdown","metadata":{"id":"wdBTKL_mqFlU"},"source":["# load the data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:07:58.193031Z","iopub.status.busy":"2024-10-06T14:07:58.192597Z","iopub.status.idle":"2024-10-06T14:07:58.201347Z","shell.execute_reply":"2024-10-06T14:07:58.199897Z","shell.execute_reply.started":"2024-10-06T14:07:58.192988Z"},"id":"sEwlgRc8p6s7","trusted":true},"outputs":[],"source":["import os\n","from pathlib import Path\n","import pandas as pd\n","import torch\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from transformers import BertTokenizer\n","from torch.utils.data import Dataset\n","from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:00:08.799569Z","iopub.status.busy":"2024-10-06T14:00:08.799209Z","iopub.status.idle":"2024-10-06T14:00:14.105310Z","shell.execute_reply":"2024-10-06T14:00:14.104144Z","shell.execute_reply.started":"2024-10-06T14:00:08.799535Z"},"id":"PlKZumtOO4vz","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# 加载数据集\n","current_path = os.path.realpath(__file__)\n","father_path = os.path.abspath(os.path.dirname(current_path)+os.path.sep+\".\")\n","data_path = Path(father_path, \"fakenewskdd2020\")\n","data = pd.read_csv(Path(data_path, 'train.csv'), sep='\\t', encoding='utf-8')\n","\n","# 去掉错误值\n","data = data.loc[data['label'].isin(['0', '1'])]\n","data = data.loc[data['text'].str.len() >=10]\n","\n","# 处理缺失值\n","data.dropna(subset=['text', 'label'], inplace=True)\n","\n","# 去掉重复数据\n","data.drop_duplicates(subset=[\"text\"], inplace=True)\n","data.reset_index()\n","\n","# 标签保证是int值\n","data['label'] = pd.to_numeric(data['label'], errors='coerce')\n","data['label'] = data['label'].astype(int)\n","                            \n","# 去除停用词\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","data['clean_text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n","\n","# 拆分训练数据\n","train_texts, test_texts, train_labels, test_labels = train_test_split(data['clean_text'], data['label'], test_size=0.2, random_state=42)\n","\n","# 用BERT进行分词向量化\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 转换成输入token\n","max_length = 128\n","train_encodings = tokenizer(list(train_texts),     \n","                            add_special_tokens=True,  # 添加 [CLS] 和 [SEP] 标记\n","                            max_length=max_length,  # 填充和截断到最大长度\n","                            padding='max_length',  # 填充到最大长度\n","                            return_attention_mask=True,  # 返回注意力掩码\n","                            return_tensors='pt',  # 返回PyTorch张量\n","                            truncation=True)\n","test_encodings = tokenizer(list(test_texts),\n","                           add_special_tokens=True,  # 添加 [CLS] 和 [SEP] 标记\n","                           max_length=max_length,  # 填充和截断到最大长度\n","                           padding='max_length',  # 填充到最大长度\n","                           return_attention_mask=True,  # 返回注意力掩码\n","                           return_tensors='pt',  # 返回PyTorch张量\n","                           truncation=True)\n","\n","# 对标签进行torch向量化转换\n","train_labels = torch.tensor(list(train_labels))\n","test_labels = torch.tensor(list(test_labels))\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:00:14.107755Z","iopub.status.busy":"2024-10-06T14:00:14.107272Z","iopub.status.idle":"2024-10-06T14:00:14.116675Z","shell.execute_reply":"2024-10-06T14:00:14.115253Z","shell.execute_reply.started":"2024-10-06T14:00:14.107701Z"},"trusted":true},"outputs":[],"source":["# \"创建一个PyTorch dataset 来使 BERT可以正常读取. 这个数据集包含tokenized和label.\"\n","class TextDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","train_dataset = TextDataset(train_encodings, train_labels)\n","test_dataset = TextDataset(test_encodings, test_labels)\n"]},{"cell_type":"markdown","metadata":{},"source":["4. Build and Fine-tune BERT Model:\n","\n","\n","Next, we fine-tune BERT using the transformers library."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:00:14.118775Z","iopub.status.busy":"2024-10-06T14:00:14.118395Z","iopub.status.idle":"2024-10-06T14:00:19.721560Z","shell.execute_reply":"2024-10-06T14:00:19.720266Z","shell.execute_reply.started":"2024-10-06T14:00:14.118735Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# 加载以BERT为底座的二分类模型\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# 设置训练参数\n","training_args = TrainingArguments(\n","    output_dir='./results',          # Output directory\n","    num_train_epochs=3,              # Number of epochs\n","    per_device_train_batch_size=16,  # Batch size for training\n","    per_device_eval_batch_size=16,   # Batch size for evaluation\n","    warmup_steps=50,                # Warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # Weight decay\n","    evaluation_strategy=\"epoch\",     # Evaluate after every epoch\n","    logging_dir='./logs',            # Directory for storing logs\n","    logging_steps=10,\n",")\n","\n","# 初始化模型\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:00:24.648388Z","iopub.status.busy":"2024-10-06T14:00:24.647886Z","iopub.status.idle":"2024-10-06T14:06:34.750436Z","shell.execute_reply":"2024-10-06T14:06:34.749102Z","shell.execute_reply.started":"2024-10-06T14:00:24.648340Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.3 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"]},{"data":{"text/html":["\n","                Tracking run with wandb version 0.10.26<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">./results</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/smalldatamatrix/huggingface\" target=\"_blank\">https://wandb.ai/smalldatamatrix/huggingface</a><br/>\n","                Run page: <a href=\"https://wandb.ai/smalldatamatrix/huggingface/runs/1h94fb2w\" target=\"_blank\">https://wandb.ai/smalldatamatrix/huggingface/runs/1h94fb2w</a><br/>\n","                Run data is saved locally in <code>/kaggle/working/wandb/run-20241006_140128-1h94fb2w</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 04:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.750996</td>\n","      <td>7.061400</td>\n","      <td>2.832000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.719700</td>\n","      <td>0.695395</td>\n","      <td>7.179800</td>\n","      <td>2.786000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.719700</td>\n","      <td>0.668804</td>\n","      <td>7.136800</td>\n","      <td>2.802000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=15, training_loss=0.7108314196268718, metrics={'train_runtime': 369.5879, 'train_samples_per_second': 0.041, 'total_flos': 20180049960960.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': -111284224, 'init_mem_cpu_peaked_delta': 111284224, 'train_mem_cpu_alloc_delta': 5490667520, 'train_mem_cpu_peaked_delta': 144232448})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 开始训练\n","trainer.train()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:12:21.758196Z","iopub.status.busy":"2024-10-06T14:12:21.757723Z","iopub.status.idle":"2024-10-06T14:12:36.819337Z","shell.execute_reply":"2024-10-06T14:12:36.817953Z","shell.execute_reply.started":"2024-10-06T14:12:21.758156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluation loss: 0.6688035726547241\n","Accuracy: 0.7\n","Precision: 0.6875\n","Recall: 0.9166666666666666\n","F1 Score: 0.7857142857142857\n"]}],"source":["# 评估模型\n","results = trainer.evaluate()\n","print(f\"Evaluation loss: {results['eval_loss']}\")\n","\n","# 验证集\n","predictions, labels, _ = trainer.predict(test_dataset)\n","predictions = torch.argmax(torch.tensor(predictions), axis=1)\n","\n","# 计算性能指标\n","accuracy = accuracy_score(test_labels, predictions)\n","precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1 Score: {f1}\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1309478,"sourceId":21237,"sourceType":"competition"}],"dockerImageVersionId":30096,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
