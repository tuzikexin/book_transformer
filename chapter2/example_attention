import torch
from tt import MultiHeadAttention


# ============================ Example usage: ============================
batch_size = 2
seq_len = 5
word_emb_d = 256
d_model = 512
num_heads = 4
dummy_input = torch.rand(batch_size, seq_len, word_emb_d)

multi_head_attn = MultiHeadAttention(word_emb_d=word_emb_d, d_model=d_model, num_heads=num_heads)
output, attn_weights = multi_head_attn(dummy_input, dummy_input, dummy_input)

print("Output shape:", output.shape)
print("Attention weights shape:", attn_weights.shape)
